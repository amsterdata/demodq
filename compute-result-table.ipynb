{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177b953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "folder = '/Users/guha/workspace/schelterlabs/CleanML/result-wagyu_20230719'\n",
    "cardio_age_threshold = 'age@45'\n",
    "\n",
    "# with open(f'{folder}/Credit_result.json') as f:\n",
    "#     credit_data = json.load(f)\n",
    "\n",
    "with open(f'{folder}/USCensus_result.json') as f:\n",
    "    adult_data = json.load(f)    \n",
    "    \n",
    "with open(f'{folder}/ACSIncome_result.json') as f:\n",
    "    folk_data = json.load(f)        \n",
    "    \n",
    "with open(f'{folder}/Cardio-{cardio_age_threshold}_result.json') as f:\n",
    "    heart_data = json.load(f)     \n",
    "    \n",
    "with open(f'{folder}/GermanCredit_result.json') as f:\n",
    "    german_data = json.load(f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc860eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def t_test(dirty, clean):\n",
    "    \"\"\"Comparing method\"\"\"\n",
    "    def two_tailed_t_test(dirty, clean):\n",
    "        n_d = len(dirty)\n",
    "        n_c = len(clean)\n",
    "        n = min(n_d, n_c)\n",
    "        t, p = ttest_rel(clean[:n], dirty[:n])\n",
    "        if np.isnan(t):\n",
    "            t, p = 0, 1\n",
    "        return {\"t-stats\":t, \"p-value\":p}\n",
    "\n",
    "    def one_tailed_t_test(dirty, clean, direction):\n",
    "        two_tail = two_tailed_t_test(dirty, clean)\n",
    "        t, p_two = two_tail['t-stats'], two_tail['p-value']\n",
    "        if direction == 'positive':\n",
    "            if t > 0 :\n",
    "                p = p_two * 0.5\n",
    "            else:\n",
    "                p = 1 - p_two * 0.5\n",
    "        else:\n",
    "            if t < 0:\n",
    "                p = p_two * 0.5\n",
    "            else:\n",
    "                p = 1 - p_two * 0.5\n",
    "        return {\"t-stats\":t, \"p-value\":p}\n",
    "\n",
    "    result = {}\n",
    "    result['two_tail'] = two_tailed_t_test(dirty, clean)\n",
    "    result['one_tail_pos'] = one_tailed_t_test(dirty, clean, 'positive')\n",
    "    result['one_tail_neg'] = one_tailed_t_test(dirty, clean, 'negative')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e676af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eo(stats, cleaning_method, criteria, flipped):\n",
    "    prefix = f'{cleaning_method}__{criteria}_'\n",
    "    \n",
    "    if not flipped:                    \n",
    "        return (stats[f'{prefix}priv__tp'] / (stats[f'{prefix}priv__tp'] + stats[f'{prefix}priv__fn'])) - \\\n",
    "             (stats[f'{prefix}dis__tp'] / (stats[f'{prefix}dis__tp'] + stats[f'{prefix}dis__fn']))\n",
    "    else:\n",
    "        return (stats[f'{prefix}priv__tn'] / (stats[f'{prefix}priv__tn'] + stats[f'{prefix}priv__fp'])) - \\\n",
    "             (stats[f'{prefix}dis__tn'] / (stats[f'{prefix}dis__tn'] + stats[f'{prefix}dis__fp']))    \n",
    "\n",
    "def compute_pp(stats, cleaning_method, criteria, flipped):\n",
    "    prefix = f'{cleaning_method}__{criteria}_'\n",
    "    \n",
    "    if not flipped:                    \n",
    "        return (stats[f'{prefix}priv__tp'] / (stats[f'{prefix}priv__tp'] + stats[f'{prefix}priv__fp'])) - \\\n",
    "             (stats[f'{prefix}dis__tp'] / (stats[f'{prefix}dis__tp'] + stats[f'{prefix}dis__fp']))\n",
    "    else:\n",
    "        return (stats[f'{prefix}priv__tn'] / (stats[f'{prefix}priv__tn'] + stats[f'{prefix}priv__fn'])) - \\\n",
    "             (stats[f'{prefix}dis__tn'] / (stats[f'{prefix}dis__tn'] + stats[f'{prefix}dis__fn']))        \n",
    "    \n",
    "\n",
    "def count(data, dataset_name, target_criteria, error_type, model, metric_name, scoring, log_file, flipped=False):\n",
    "\n",
    "    dirty_scores = []\n",
    "    dirty_accs = []\n",
    "    \n",
    "    cleaning_scores = {}\n",
    "    cleaning_accs = {}\n",
    "    \n",
    "    dirty = 'dirty'\n",
    "    if error_type == 'missing_values':\n",
    "        dirty = 'delete'\n",
    "    \n",
    "    for experiment in data.keys():\n",
    "        if error_type in experiment and model in experiment:        \n",
    "            split_seed = experiment.split(\"/\")[1]\n",
    "            train_method = experiment.split(\"/\")[3]   \n",
    "            retrain_seed = experiment.split(\"/\")[5]\n",
    "\n",
    "            if train_method == dirty:\n",
    "                # Missing values need special treatment, just deleting the corresponding rows from the test set\n",
    "                # is not applicable in real-world scenarios, so we set a default way to treat the test data\n",
    "                if error_type == 'missing_values':\n",
    "                    score = scoring(data[experiment], 'impute_mean_dummy', target_criteria, flipped)\n",
    "                    dirty_scores.append(score)                        \n",
    "                    dirty_accs.append(data[experiment]['impute_mean_dummy_test_acc'])                    \n",
    "                else:\n",
    "                    score = scoring(data[experiment], dirty, target_criteria, flipped)\n",
    "                    dirty_scores.append(score)                        \n",
    "                    dirty_accs.append(data[experiment][dirty + '_test_acc'])\n",
    "\n",
    "\n",
    "            if train_method != dirty:\n",
    "\n",
    "                for test_method in [dirty, train_method]:\n",
    "                    \n",
    "                    approach = (train_method, test_method)\n",
    "                    \n",
    "                    if approach not in cleaning_scores:\n",
    "                        cleaning_scores[approach] = []\n",
    "\n",
    "                    if error_type == 'mislabel':\n",
    "                        scores = scoring(data[experiment], 'clean', target_criteria, flipped)\n",
    "                    else:     \n",
    "                        scores = scoring(data[experiment], test_method, target_criteria, flipped)\n",
    "                    cleaning_scores[approach].append(scores)\n",
    "\n",
    "                    if approach not in cleaning_accs:\n",
    "                        cleaning_accs[approach] = []\n",
    "\n",
    "                    if test_method == dirty:\n",
    "                        cleaning_accs[approach].append(data[experiment][f'{dirty}_test_acc'])\n",
    "                    else:    \n",
    "                        if error_type != 'mislabel':\n",
    "                            cleaning_accs[approach].append(data[experiment][f'{train_method}_test_acc'])\n",
    "                        \n",
    "    evaluate_scores(dirty_scores, cleaning_scores, dirty_accs, cleaning_accs, \n",
    "                    dataset_name, target_criteria, metric_name, model, error_type, log_file)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a510b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scores(dirty_scores, cleaning_scores, dirty_accs, cleaning_accs, \n",
    "                    dataset_name, target_criteria, metric_name, model, error_type, log_file):\n",
    "                                                                          \n",
    "    if len(cleaning_scores) > 0:        \n",
    "            \n",
    "        # bonferroni correction \n",
    "        alpha = 0.05 / len(cleaning_scores)\n",
    "\n",
    "        for method, scores in cleaning_scores.items():\n",
    "            \n",
    "            test_results = t_test(dirty_scores, scores)\n",
    "\n",
    "            repair_train, repair_clean = method\n",
    "            test_repaired = repair_train == repair_clean\n",
    "\n",
    "            difference = 'insignificant'\n",
    "\n",
    "            if test_results['two_tail']['p-value'] < alpha:\n",
    "                if test_results['one_tail_neg']['p-value'] < alpha:\n",
    "                    difference = 'positive'\n",
    "                if test_results['one_tail_pos']['p-value'] < alpha:\n",
    "                    difference = 'negative'\n",
    "\n",
    "                    \n",
    "            acc_test_results = t_test(dirty_accs, cleaning_accs[method])\n",
    "\n",
    "            acc_difference = 'insignificant'\n",
    "\n",
    "            if acc_test_results['two_tail']['p-value'] < alpha:\n",
    "                if acc_test_results['one_tail_neg']['p-value'] < alpha:\n",
    "                    acc_difference = 'negative'\n",
    "                if acc_test_results['one_tail_pos']['p-value'] < alpha:\n",
    "                    acc_difference = 'positive'                                  \n",
    "                        \n",
    "            if error_type == 'missing_values':\n",
    "                repair_method = repair_train\n",
    "                detection=''\n",
    "            elif error_type == 'mislabel':\n",
    "                tokens = repair_train.split('-')\n",
    "                repair_method = tokens[1]\n",
    "                detection=tokens[0]\n",
    "            else:    \n",
    "                tokens = repair_train.split(\"_impute\")        \n",
    "                detection = tokens[0].replace('clean_', '')\n",
    "                repair_method = 'impute_' + tokens[1]\n",
    "                \n",
    "            if not (error_type == 'mislabel' and (test_repaired or detection == 'shapley')):        \n",
    "                          \n",
    "                line = f'{dataset_name},{target_criteria},{metric_name},{model},{error_type},{detection},{repair_method},{test_repaired},{difference},{acc_difference}'    \n",
    "\n",
    "                if test_repaired or error_type == 'mislabel':\n",
    "                    print(line)            \n",
    "                    log_file.write(f'{line}\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4f5bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult,sex,equal_opportunity,logistic_regression,outliers,SD,impute__mean_dummy,True,insignificant,positive\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,SD,impute__mode_dummy,True,insignificant,positive\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,SD,impute__median_dummy,True,insignificant,positive\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,IQR,impute__mean_dummy,True,negative,positive\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,IQR,impute__mode_dummy,True,negative,insignificant\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,IQR,impute__median_dummy,True,negative,insignificant\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,IF,impute__mean_dummy,True,negative,positive\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,IF,impute__mode_dummy,True,insignificant,positive\n",
      "adult,sex,equal_opportunity,logistic_regression,outliers,IF,impute__median_dummy,True,insignificant,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,SD,impute__mean_dummy,True,positive,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,SD,impute__mode_dummy,True,positive,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,SD,impute__median_dummy,True,positive,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,IQR,impute__mean_dummy,True,negative,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,IQR,impute__mode_dummy,True,negative,insignificant\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,IQR,impute__median_dummy,True,negative,insignificant\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,IF,impute__mean_dummy,True,positive,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,IF,impute__mode_dummy,True,positive,positive\n",
      "adult,race,equal_opportunity,logistic_regression,outliers,IF,impute__median_dummy,True,positive,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,SD,impute__mean_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,SD,impute__mode_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,SD,impute__median_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,IQR,impute__mean_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,IQR,impute__mode_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,IQR,impute__median_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,IF,impute__mean_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,IF,impute__mode_dummy,True,negative,positive\n",
      "folktables,sex,equal_opportunity,logistic_regression,outliers,IF,impute__median_dummy,True,negative,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,SD,impute__mean_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,SD,impute__mode_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,SD,impute__median_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,IQR,impute__mean_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,IQR,impute__mode_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,IQR,impute__median_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,IF,impute__mean_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,IF,impute__mode_dummy,True,insignificant,positive\n",
      "folktables,rac1p,equal_opportunity,logistic_regression,outliers,IF,impute__median_dummy,True,insignificant,positive\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'credit_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m count(folk_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolktables\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, error, model, metric, scoring, log_file)\n\u001b[1;32m     16\u001b[0m count(folk_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolktables\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrac1p\u001b[39m\u001b[38;5;124m'\u001b[39m, error, model, metric, scoring, log_file)            \n\u001b[0;32m---> 17\u001b[0m count(\u001b[43mcredit_data\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, error, model, metric, scoring, log_file, flipped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m count(german_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgerman\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, error, model, metric, scoring, log_file, flipped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n\u001b[1;32m     19\u001b[0m count(heart_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheart\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m'\u001b[39m, error, model, metric, scoring, log_file, flipped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)              \n",
      "\u001b[0;31mNameError\u001b[0m: name 'credit_data' is not defined"
     ]
    }
   ],
   "source": [
    "errors = ['outliers', 'missing_values', 'mislabel']\n",
    "models = ['logistic_regression', 'knn_classification', 'XGBoost']\n",
    "metrics = [('equal_opportunity', compute_eo), ('predictive_parity', compute_pp)]\n",
    "\n",
    "\n",
    "with open('cleanml_20230727.csv', 'w') as log_file:\n",
    "    \n",
    "    log_file.write('dataset,criteria,metric,model,error,detection,repair,test_repaired,fairness_impact,accuracy_impact\\n')\n",
    "    \n",
    "    for metric, scoring in metrics:\n",
    "        for error in errors:    \n",
    "            for model in models:\n",
    "                count(adult_data, 'adult', 'sex', error, model, metric, scoring, log_file)\n",
    "                count(adult_data, 'adult', 'race', error, model, metric, scoring, log_file)\n",
    "                count(folk_data, 'folktables', 'sex', error, model, metric, scoring, log_file)\n",
    "                count(folk_data, 'folktables', 'rac1p', error, model, metric, scoring, log_file)            \n",
    "                count(credit_data, 'credit', 'age', error, model, metric, scoring, log_file, flipped=True)\n",
    "                count(german_data, 'german', 'age', error, model, metric, scoring, log_file, flipped=True)  \n",
    "                count(heart_data, 'heart', 'gender', error, model, metric, scoring, log_file, flipped=False)              \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
