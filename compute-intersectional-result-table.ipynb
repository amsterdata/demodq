{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "177b953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "folder = '/Users/guha/workspace/schelterlabs/CleanML/result'\n",
    "\n",
    "# with open(f'{folder}/Credit_result.json') as f:\n",
    "#     credit_data = json.load(f)\n",
    "\n",
    "with open(f'{folder}/USCensus_result.json') as f:\n",
    "    adult_data = json.load(f)\n",
    "\n",
    "# with open(f'{folder}/ACSIncome_result.json') as f:\n",
    "#     folk_data = json.load(f)\n",
    "\n",
    "# with open(f'{folder}/Cardio_result.json') as f:\n",
    "#     heart_data = json.load(f)\n",
    "\n",
    "# with open(f'{folder}/GermanCredit_result.json') as f:\n",
    "#     german_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc860eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def t_test(dirty, clean):\n",
    "    \"\"\"Comparing method\"\"\"\n",
    "    def two_tailed_t_test(dirty, clean):\n",
    "        n_d = len(dirty)\n",
    "        n_c = len(clean)\n",
    "        n = min(n_d, n_c)\n",
    "        t, p = ttest_rel(clean[:n], dirty[:n])\n",
    "        if np.isnan(t):\n",
    "            t, p = 0, 1\n",
    "        return {\"t-stats\":t, \"p-value\":p}\n",
    "\n",
    "    def one_tailed_t_test(dirty, clean, direction):\n",
    "        two_tail = two_tailed_t_test(dirty, clean)\n",
    "        t, p_two = two_tail['t-stats'], two_tail['p-value']\n",
    "        if direction == 'positive':\n",
    "            if t > 0 :\n",
    "                p = p_two * 0.5\n",
    "            else:\n",
    "                p = 1 - p_two * 0.5\n",
    "        else:\n",
    "            if t < 0:\n",
    "                p = p_two * 0.5\n",
    "            else:\n",
    "                p = 1 - p_two * 0.5\n",
    "        return {\"t-stats\":t, \"p-value\":p}\n",
    "\n",
    "    result = {}\n",
    "    result['two_tail'] = two_tailed_t_test(dirty, clean)\n",
    "    result['one_tail_pos'] = one_tailed_t_test(dirty, clean, 'positive')\n",
    "    result['one_tail_neg'] = one_tailed_t_test(dirty, clean, 'negative')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e676af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix(cleaning_method, criteria, intersectional_group):\n",
    "    if intersectional_group == \"priv_priv\":\n",
    "        return f'{cleaning_method}__{criteria[0]}_priv__{criteria[1]}_priv'\n",
    "    elif intersectional_group == \"priv_dis\":\n",
    "        return f'{cleaning_method}__{criteria[0]}_priv__{criteria[1]}_dis'\n",
    "    elif intersectional_group == \"dis_priv\":\n",
    "        return f'{cleaning_method}__{criteria[0]}_dis__{criteria[1]}_priv'\n",
    "    elif intersectional_group == \"dis_dis\":\n",
    "        return f'{cleaning_method}__{criteria[0]}_dis__{criteria[1]}_dis'\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported: {intersectional_group}\")\n",
    "\n",
    "def compute_eo(stats, cleaning_method, criteria, flipped):\n",
    "    # TODO: Define priv and dis intersectional groups\n",
    "    # e.g. priv = intersectionally priv, i.e. priv_priv\n",
    "    #      dis = intersectionally dis, i.e. dis_dis\n",
    "    # TODO: Implement one vs. everyone, aggregating across multiple intersectional groups\n",
    "    priv_prefix = prefix(cleaning_method, criteria, \"priv_priv\")\n",
    "    dis_prefix = prefix(cleaning_method, criteria, \"dis_dis\")\n",
    "\n",
    "    if not flipped:\n",
    "        return (stats[f'{priv_prefix}__tp'] / (stats[f'{priv_prefix}__tp'] + stats[f'{priv_prefix}__fn'])) - \\\n",
    "             (stats[f'{dis_prefix}__tp'] / (stats[f'{dis_prefix}__tp'] + stats[f'{dis_prefix}__fn']))\n",
    "    else:\n",
    "        return (stats[f'{priv_prefix}__tn'] / (stats[f'{priv_prefix}__tn'] + stats[f'{priv_prefix}__fp'])) - \\\n",
    "             (stats[f'{dis_prefix}__tn'] / (stats[f'{dis_prefix}__tn'] + stats[f'{dis_prefix}__fp']))\n",
    "\n",
    "def compute_pp(stats, cleaning_method, criteria, flipped):\n",
    "    # TODO: Define priv and dis intersectional groups\n",
    "    # e.g. priv = intersectionally priv, i.e. priv_priv\n",
    "    #      dis = intersectionally dis, i.e. dis_dis\n",
    "    # TODO: Implement one vs. everyone, aggregating across multiple intersectional groups\n",
    "    priv_prefix = prefix(cleaning_method, criteria, \"priv_priv\")\n",
    "    dis_prefix = prefix(cleaning_method, criteria, \"dis_dis\")\n",
    "\n",
    "    if not flipped:\n",
    "        return (stats[f'{priv_prefix}__tp'] / (stats[f'{priv_prefix}__tp'] + stats[f'{priv_prefix}__fp'])) - \\\n",
    "             (stats[f'{dis_prefix}__tp'] / (stats[f'{dis_prefix}__tp'] + stats[f'{dis_prefix}__fp']))\n",
    "    else:\n",
    "        return (stats[f'{priv_prefix}__tn'] / (stats[f'{priv_prefix}__tn'] + stats[f'{priv_prefix}__fn'])) - \\\n",
    "             (stats[f'{dis_prefix}__tn'] / (stats[f'{dis_prefix}__tn'] + stats[f'{dis_prefix}__fn']))\n",
    "\n",
    "def count(data, dataset_name, target_criteria, error_type, model, metric_name, scoring, log_file, flipped=False):\n",
    "    dirty_scores = []\n",
    "    dirty_accs = []\n",
    "\n",
    "    cleaning_scores = {}\n",
    "    cleaning_accs = {}\n",
    "\n",
    "    dirty = 'dirty'\n",
    "    if error_type == 'missing_values':\n",
    "        dirty = 'delete'\n",
    "\n",
    "    for experiment in data:\n",
    "        if error_type in experiment and model in experiment:\n",
    "            split_seed = experiment.split(\"/\")[1]\n",
    "            train_method = experiment.split(\"/\")[3]\n",
    "            retrain_seed = experiment.split(\"/\")[5]\n",
    "\n",
    "            if train_method == dirty:\n",
    "                # Missing values need special treatment, just deleting the corresponding rows from the test set\n",
    "                # is not applicable in real-world scenarios, so we set a default way to treat the test data\n",
    "                if error_type == 'missing_values':\n",
    "                    score = scoring(data[experiment], 'impute_mean_dummy', target_criteria, flipped)\n",
    "                    dirty_scores.append(score)\n",
    "                    dirty_accs.append(data[experiment]['impute_mean_dummy_test_acc'])\n",
    "                else:\n",
    "                    score = scoring(data[experiment], dirty, target_criteria, flipped)\n",
    "                    dirty_scores.append(score)\n",
    "                    dirty_accs.append(data[experiment][dirty + '_test_acc'])\n",
    "\n",
    "            if train_method != dirty:\n",
    "                for test_method in [dirty, train_method]:\n",
    "                    approach = (train_method, test_method)\n",
    "\n",
    "                    if approach not in cleaning_scores:\n",
    "                        cleaning_scores[approach] = []\n",
    "\n",
    "                    if error_type == 'mislabel':\n",
    "                        scores = scoring(data[experiment], 'clean', target_criteria, flipped)\n",
    "                    else:\n",
    "                        scores = scoring(data[experiment], test_method, target_criteria, flipped)\n",
    "                    cleaning_scores[approach].append(scores)\n",
    "\n",
    "                    if approach not in cleaning_accs:\n",
    "                        cleaning_accs[approach] = []\n",
    "\n",
    "                    if test_method == dirty:\n",
    "                        cleaning_accs[approach].append(data[experiment][f'{dirty}_test_acc'])\n",
    "                    else:\n",
    "                        if error_type != 'mislabel':\n",
    "                            cleaning_accs[approach].append(data[experiment][f'{train_method}_test_acc'])\n",
    "\n",
    "    evaluate_scores(dirty_scores, cleaning_scores, dirty_accs, cleaning_accs,\n",
    "                    dataset_name, target_criteria, metric_name, model, error_type, log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a510b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scores(dirty_scores, cleaning_scores, dirty_accs, cleaning_accs,\n",
    "                    dataset_name, target_criteria, metric_name, model, error_type, log_file):\n",
    "    if len(cleaning_scores) > 0:\n",
    "        # bonferroni correction\n",
    "        alpha = 0.05 / len(cleaning_scores)\n",
    "\n",
    "        for method, scores in cleaning_scores.items():\n",
    "            test_results = t_test(dirty_scores, scores)\n",
    "\n",
    "            repair_train, repair_clean = method\n",
    "            test_repaired = repair_train == repair_clean\n",
    "\n",
    "            difference = 'insignificant'\n",
    "\n",
    "            if test_results['two_tail']['p-value'] < alpha:\n",
    "                if test_results['one_tail_neg']['p-value'] < alpha:\n",
    "                    difference = 'positive'\n",
    "                if test_results['one_tail_pos']['p-value'] < alpha:\n",
    "                    difference = 'negative'\n",
    "\n",
    "            acc_test_results = t_test(dirty_accs, cleaning_accs[method])\n",
    "\n",
    "            acc_difference = 'insignificant'\n",
    "\n",
    "            if acc_test_results['two_tail']['p-value'] < alpha:\n",
    "                if acc_test_results['one_tail_neg']['p-value'] < alpha:\n",
    "                    acc_difference = 'negative'\n",
    "                if acc_test_results['one_tail_pos']['p-value'] < alpha:\n",
    "                    acc_difference = 'positive'\n",
    "\n",
    "            if error_type == 'missing_values':\n",
    "                repair_method = repair_train\n",
    "                detection=''\n",
    "            elif error_type == 'mislabel':\n",
    "                tokens = repair_train.split('-')\n",
    "                repair_method = tokens[1]\n",
    "                detection=tokens[0]\n",
    "            else:\n",
    "                tokens = repair_train.split(\"_impute\")\n",
    "                detection = tokens[0].replace('clean_', '')\n",
    "                repair_method = 'impute_' + tokens[1]\n",
    "\n",
    "            if not (error_type == 'mislabel' and (test_repaired or detection == 'shapley')):\n",
    "                line = f'{dataset_name},{\"/\".join(target_criteria)},{metric_name},{model},{error_type},{detection},{repair_method},{test_repaired},{difference},{acc_difference}'\n",
    "\n",
    "                if test_repaired or error_type == 'mislabel':\n",
    "                    print(line)\n",
    "                    log_file.write(f'{line}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be4f5bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult,sex/race,equal_opportunity,logistic_regression,missing_values,,impute_mean_mode,True,negative,negative\n",
      "adult,sex/race,equal_opportunity,logistic_regression,missing_values,,impute_mean_dummy,True,negative,negative\n",
      "adult,sex/race,equal_opportunity,knn_classification,missing_values,,impute_mean_mode,True,insignificant,negative\n",
      "adult,sex/race,equal_opportunity,knn_classification,missing_values,,impute_mean_dummy,True,insignificant,insignificant\n",
      "adult,sex/race,equal_opportunity,XGBoost,missing_values,,impute_mean_mode,True,positive,negative\n",
      "adult,sex/race,predictive_parity,logistic_regression,missing_values,,impute_mean_mode,True,positive,negative\n",
      "adult,sex/race,predictive_parity,logistic_regression,missing_values,,impute_mean_dummy,True,positive,negative\n",
      "adult,sex/race,predictive_parity,knn_classification,missing_values,,impute_mean_mode,True,insignificant,negative\n",
      "adult,sex/race,predictive_parity,knn_classification,missing_values,,impute_mean_dummy,True,insignificant,insignificant\n",
      "adult,sex/race,predictive_parity,XGBoost,missing_values,,impute_mean_mode,True,negative,negative\n"
     ]
    }
   ],
   "source": [
    "errors = ['outliers', 'missing_values', 'mislabel']\n",
    "models = ['logistic_regression', 'knn_classification', 'XGBoost']\n",
    "metrics = [('equal_opportunity', compute_eo), ('predictive_parity', compute_pp)]\n",
    "\n",
    "with open('cleanml.csv', 'w') as log_file:\n",
    "    log_file.write('dataset,criteria,metric,model,error,detection,repair,test_repaired,fairness_impact,accuracy_impact\\n')\n",
    "\n",
    "    for metric, scoring in metrics:\n",
    "        for error in errors:\n",
    "            for model in models:\n",
    "                count(adult_data, 'adult', ('sex', 'race'), error, model, metric, scoring, log_file)\n",
    "#                 count(adult_data, 'adult', 'sex', error, model, metric, scoring, log_file)\n",
    "#                 count(adult_data, 'adult', 'race', error, model, metric, scoring, log_file)\n",
    "#                 count(folk_data, 'folktables', 'sex', error, model, metric, scoring, log_file)\n",
    "#                 count(folk_data, 'folktables', 'rac1p', error, model, metric, scoring, log_file)\n",
    "#                 count(credit_data, 'credit', 'age', error, model, metric, scoring, log_file, flipped=True)\n",
    "#                 count(german_data, 'german', 'age', error, model, metric, scoring, log_file, flipped=True)\n",
    "#                 count(heart_data, 'heart', 'gender', error, model, metric, scoring, log_file, flipped=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
